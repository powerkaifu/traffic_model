# main.py
import numpy as np
import pandas as pd
import tensorflow as tf
import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model  # type: ignore

from data_process import combine_vd_dataframes
from preprocess import preprocess_data
from predictor import build_model, train_model, evaluate_model, predict_new
from visualizer import plot_volume_distribution, plot_speed_distribution, plot_residuals

gpus = tf.config.list_physical_devices('GPU')
print('1111111111', tf.test.is_built_with_cuda())
print('222222222', tf.test.is_gpu_available())

pd.set_option('display.width', None)
pd.set_option('display.max_columns', None)

# è¨­å®š VD è³‡æ–™å¤¾å’Œæª”æ¡ˆåç¨±
base_dir = "."
vd_folders = [ 'VLRJM60', 'VLRJX00', 'VLRJX20']
date_file = '2025-05-05_2025-05-11.json'
# date_file = 'test.json' # æ¸¬è©¦ç”¨çš„æª”æ¡ˆï¼Œå¸Œæœ›æ¨¡æ“¬å‡ºç‰¹æ®Šæƒ…æ³ï¼Œä¾‹å¦‚å¤©æ°£å½±éŸ¿ã€äº¤é€šäº‹æ•…çš„ç‰¹å¾µè³‡æ–™

# è®€å–ä¸¦åˆä½µ VD è³‡æ–™
merged_df = combine_vd_dataframes(base_dir, vd_folders, date_file)
print(f"åˆä½µå¾Œçš„ DataFrame è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)}")
print(f"åˆä½µå¾Œçš„ DataFrame æ¬„ä½ï¼š{merged_df.columns}")
# print(f"åˆä½µå¾Œçš„ DataFrame å‰äº”ç­†ï¼š", merged_df.head())

# ç¹ªåœ–-è£½æµé‡å’Œé€Ÿåº¦çš„åˆ†å¸ƒåœ– ------------------------------------------------------------------
# plot_volume_distribution(merged_df)
# plot_speed_distribution(merged_df)

# ----------------------------------------------------------------------------------------

if merged_df is not None:
  print("=" * 80)

  # å›å‚³ X, y å’ŒåŸå§‹ç´¢å¼•
  X, y, original_indices = preprocess_data(merged_df, return_indices = True)

  # è³‡æ–™è½‰å‹
  X = X.astype(float)
  y = y.astype(float)
  print("æŸ¥çœ‹ X å’Œ y çš„è³‡æ–™å‹æ…‹ï¼š")
  print(type(X), X.dtype, X.shape)
  print(type(y), y.dtype, y.shape)
  print("X ç‰¹å¾µè³‡æ–™é›†ï¼š", X[0])
  print("y ç›®æ¨™è®Šæ•¸è³‡æ–™é›†ï¼š", y[0])
  print("=" * 80)

  # âœ… æ§åˆ¶æ˜¯å¦ä½¿ç”¨è¨“ç·´é›† / æ¸¬è©¦é›†åˆ†å‰² -------------------------------------------------------------------------------------------
  use_split = False  # True ä½¿ç”¨åˆ†å‰²ï¼›False ä½¿ç”¨å…¨éƒ¨è³‡æ–™è¨“ç·´

  if use_split:
    # åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™ï¼ˆå«åŸå§‹ç´¢å¼•ï¼‰
    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, original_indices, test_size = 0.2, random_state = 42)
    print("ğŸ“¦ ä½¿ç”¨åˆ†å‰²æ–¹å¼è¨“ç·´")
  else:
    X_train, y_train = X, y
    X_test, y_test = X, y
    indices_train, indices_test = original_indices, original_indices
    print("ğŸ“¦ ä½¿ç”¨ä¸åˆ†å‰²æ–¹å¼è¨“ç·´")

  # å˜—è©¦è¼‰å…¥æ¨¡å‹ï¼Œè‹¥å¤±æ•—å‰‡å»ºç«‹æ–°æ¨¡å‹ ----------------------------------------------------------------------------------------------
  model_path = './traffic_models/trained_model.h5'
  try:
    if os.path.exists(model_path):
      model = load_model(model_path, custom_objects = { 'mse': tf.keras.losses.MeanSquaredError})
      print("âœ… å·²åŠ è¼‰å…ˆå‰è¨“ç·´çš„æ¨¡å‹")
      model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mse')
    else:
      raise FileNotFoundError("æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨")
  except Exception as e:
    print(f"âš ï¸ è¼‰å…¥æ¨¡å‹å¤±æ•—ï¼ŒåŸå› ï¼š{e}")
    model = build_model(input_shape = X.shape[1])  # X.shape[1] æ˜¯ç‰¹å¾µæ•¸é‡ï¼Œ20
    print("ğŸ†• å»ºç«‹æ–°çš„æ¨¡å‹")

  # æ¨¡å‹è¨“ç·´ -------------------------------------------------------------------------------------------------------------------
  train_model(model, X_train, y_train, epochs = 1)  # epochs è¨“ç·´çš„è¼ªæ•¸

  # å„²å­˜æ¨¡å‹
  os.makedirs(os.path.dirname(model_path), exist_ok = True)
  model.save(model_path)
  print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³:", model_path)

  # é æ¸¬èˆ‡è©•ä¼°
  ## å¦‚æœç”¨äº†ä¸åˆ†å‰²è³‡æ–™è¨“ç·´ï¼Œå°±ä¸éœ€è¦è©•ä¼°æ¨¡å‹ï¼Œå› ç‚ºæ²’æœ‰æ¸¬è©¦é›†ï¼Œä½†å¥½è™•å°±æ˜¯å¯ä»¥ç”¨å…¨éƒ¨è³‡æ–™è¨“ç·´æ¨¡å‹
  y_pred = evaluate_model(model, X_test, y_test)  # y_pred æ¨¡å‹é æ¸¬ç¶ ç‡ˆç§’æ•¸çš„çµæœ
  new_pred = predict_new(model, X_test)

  # ç¹ªåœ–-æ®˜å·®åœ– ----------------------------------------------------------------------------------------------------------------
  # plot_residuals(y_test, y_pred)

  # é¡¯ç¤ºé æ¸¬çµæœï¼Œä¸¦è¼¸å‡ºåˆ° output.txt --------------------------------------------------------------------------------------------
  over_seconds = 50  # é æ¸¬ç¶ ç‡ˆç§’æ•¸å¤§æ–¼ 50 ç§’
  print("=" * 80)
  print(f"é æ¸¬ç¶ ç‡ˆç§’æ•¸å¤§æ–¼ {over_seconds} ç§’çš„è³‡æ–™ï¼š")
  output_data_count = 100  # é™åˆ¶è¼¸å‡ºè³‡æ–™ç­†æ•¸
  over_predictions = [( i, pred[0] ) for i, pred in enumerate(new_pred) if pred[0] > over_seconds]

  with open("output.txt", "w", encoding = "utf-8") as f:
    for idx, ( test_index, val ) in enumerate(over_predictions):
      if idx >= output_data_count:
        break
      original_index_in_merged_df = indices_test[test_index]
      original_data = merged_df.loc[original_index_in_merged_df]
      # å¯«å…¥åˆ° output.txtï¼Œ file = f
      print(f"é æ¸¬çµæœç´¢å¼• {idx} (æ¸¬è©¦é›†ç´¢å¼•: {test_index}, åŸå§‹ DataFrame ç´¢å¼•: {original_index_in_merged_df})ï¼Œé æ¸¬ç¶ ç‡ˆç§’æ•¸ï¼š{val:.1f}", file = f)
      print("åŸå§‹è³‡æ–™:", file = f)
      print(original_data, file = f)
      print("-" * 50, file = f)

else:
  print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½• VD çš„è³‡æ–™ã€‚")
