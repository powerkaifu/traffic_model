import numpy as np
import pandas as pd
import os
import json
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model  # type: ignore

# è‡ªå·±çš„æ¨¡çµ„
from data_process import combine_vd_dataframes
from feature_engineering import prepare_features, inverse_transform_all
from predictor import build_model, train_model, evaluate_model, predict_new
from visualizer import *
from shap_utils import explain_shap_feature

pd.set_option('display.width', None)
pd.set_option('display.max_columns', None)

# ä¸»ç¨‹å¼
def main():
  base_dir = "."
  vd_folders = [ 'VLRJM60', 'VLRJX00', 'VLRJX20']
  date_file = '2025-05-05_2025-05-11.json'

  # åˆä½µå¤šå€‹åµæ¸¬å™¨è³‡æ–™ç‚ºä¸€å€‹ DataFrame
  merged_df = combine_vd_dataframes(base_dir, vd_folders, date_file)
  if merged_df is None:
    print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½• VD çš„è³‡æ–™ã€‚")
    return

  print(f"åˆä½µå¾Œçš„ DataFrame è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)}")
  print(f"åˆä½µå¾Œçš„ DataFrame æ¬„ä½ï¼š{merged_df.columns}")
  print("-" * 80)

  # é€²è¡Œç‰¹å¾µå·¥ç¨‹ --------------------------------------------------------------------------------------------------------
  ## ä¸€èˆ¬ç‰¹å¾µæ¨™æº–åŒ–çš„ X æ•¸æ“šéƒ½è½æ–¼ -3 åˆ° 3 ä¹‹é–“
  X, y, original_indices, feature_names, df = prepare_features(merged_df, is_training = True, return_indices = True)
  print(feature_names)
  if X is None or y is None or feature_names is None:
    print("âŒ è³‡æ–™å‰è™•ç†å¤±æ•—ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
    return

  # ç¢ºä¿æ˜¯ float é¡å‹
  X = X.astype(float)
  y = y.astype(float)

  # åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† -------------ã„¦------------------------------------------------------------------------------------
  use_split = True  # True ä»£è¡¨ç”¨è¨“ç·´æ¸¬è©¦åˆ†å‰²ï¼ŒFalse ä»£è¡¨å…¨éƒ¨è¨“ç·´
  if use_split:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        shuffle=False,
    )
    print("ğŸ“¦ ä½¿ç”¨åˆ†å‰²æ–¹å¼è¨“ç·´")
  else:
    X_train, y_train = X, y
    X_test, y_test = X, y
    print("ğŸ“¦ ä½¿ç”¨ä¸åˆ†å‰²æ–¹å¼è¨“ç·´")

  # è¼‰å…¥æ¨¡å‹ï¼Œå¦å‰‡å»ºç«‹æ–°æ¨¡å‹ ----------------------------------------------------------------------------------------------
  model_path = './traffic_models/trained_model.keras'  # æ¨¡å‹å„²å­˜è·¯å¾‘
  scaler_path = './traffic_models/scaler.pkl'  # ç‰¹å¾µç¸®æ”¾å™¨å„²å­˜è·¯å¾‘
  if os.path.exists(model_path):
    model = load_model(model_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰è¨“ç·´çš„æ¨¡å‹")
    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mse')
  else:
    print("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„æ¨¡å‹")
    print(f"âš ï¸ å»ºç«‹æ–°æ¨¡å‹ï¼Œè¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼š{X.shape[1]}")  # 26 å€‹ç‰¹å¾µ
    model = build_model(input_shape = X.shape[1])
    print("ğŸ†• å»ºç«‹æ–°çš„æ¨¡å‹")

  # è¼‰å…¥ scaler ç”¨æ–¼æ¨™æº–åŒ– --------------------------------------------------------------------------------------------
  scaler = None
  if os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰ä½¿ç”¨çš„ç‰¹å¾µç¸®æ”¾å™¨")
  else:
    if os.path.exists(model_path):
      print("âš ï¸ è¼‰å…¥ç‰¹å¾µç¸®æ”¾å™¨å¤±æ•—ï¼šscaler.pkl æª”æ¡ˆä¸å­˜åœ¨ã€‚")

  # è¨“ç·´æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  print("â³ é–‹å§‹æ¨¡å‹è¨“ç·´...")
  train_model(model, X_train, y_train, epochs = 50)
  print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

  # å„²å­˜æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  model.save(model_path)
  print(f"âœ… æ¨¡å‹å·²å„²å­˜åˆ° {model_path}")

  # å„²å­˜ scaler æ¨™æº–åŒ–å™¨ ----------------------------------------------------------------------------------------------
  if scaler is not None:
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜åˆ° {scaler_path}")
  else:
    print("âš ï¸ scaler ç‚ºç©ºï¼Œç„¡æ³•å„²å­˜")

  # æ¨¡å‹è©•ä¼° ---------------------------------------------------------------------------------------------------------
  print("ğŸ“Š é–‹å§‹æ¨¡å‹è©•ä¼°...")
  y_pred = evaluate_model(model, X_test, y_test)
  batch_predictions_scaled = predict_new(model, X_test)
  print("âœ… æ¨¡å‹è©•ä¼°å®Œæˆã€‚")

  # ç¹ªè£½åœ–è¡¨ ---------------------------------------------------------------------------------------------------------
  ## æ•£é»åœ–
  plot_scatter_predictions(y_test.flatten(), y_pred.flatten())

  # åæ¨™æº–åŒ– Occupancyï¼Œé€™æ‰èƒ½é¡¯ç¤ºåŸæœ¬çš„ä½”ç”¨ç‡ï¼ˆå› ç‚ºæ¨™æº–åŒ–å¾Œçš„ Occupancy æœƒè½åœ¨ -n åˆ° +n ä¹‹é–“ï¼‰
  df_viz = df.copy()
  features_to_inverse = list(scaler.feature_names_in_)
  df_viz = inverse_transform_all(df, scaler, features_to_inverse)
  # plot_occupancy_vs_green_seconds(df_viz)
  # plot_occupancy_distribution(df_viz)
  # plot_occupancy_time_trend(df_viz)

  # plot_volume_distribution(merged_df)
  # plot_speed_distribution(merged_df)
  # plot_residuals(y_test, y_pred)

  # SHAP
  # print('-' * 80)
  # explain_shap_feature(model, X_train, X_test, feature_names, output_dir = "shap")
  # print('-' * 80)

# åŸ·è¡Œä¸»ç¨‹å¼
if __name__ == "__main__":
  main()
