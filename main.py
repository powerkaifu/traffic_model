import numpy as np
import pandas as pd
import os
import json
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model  # type: ignore

# è‡ªå·±çš„æ¨¡çµ„
from data_process import combine_vd_dataframes
from feature_engineering import prepare_features, inverse_transform_all
from predictor import build_model, train_model, evaluate_model, predict_new
from visualizer import *
from shap_utils import explain_shap_feature

pd.set_option('display.width', None)
pd.set_option('display.max_columns', None)


# è™•ç†æ–°çš„è³‡æ–™ï¼Œå°‡å…¶è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥æ¥å—çš„æ ¼å¼ï¼Œä¸¦é€²è¡Œæ¨™æº–åŒ–
def preprocess_and_scale_new_data(new_data_df: pd.DataFrame, feature_names: list, scaler):
  one_hot_vd_cols = [ col for col in feature_names if col.startswith('VD_ID_') ]
  new_data_df_processed = pd.get_dummies(new_data_df, columns = ['VD_ID'], prefix = 'VD_ID')

  # è£œé½Šç¼ºå°‘çš„ one-hot æ¬„ä½
  for col in one_hot_vd_cols:
    if col not in new_data_df_processed.columns:
      new_data_df_processed[col] = 0

  numerical_features_to_scale = [
      'Speed',
      'Occupancy',
      'Volume_M',
      'Volume_S',
      'Volume_L',
      'Volume_T',
      'Speed_M',
      'Speed_S',
      'Speed_L',
      'Speed_T',
      'DayOfWeek',
      'Hour',
      'Minute',
      'Second',
      'LaneID',
      'LaneType',
      'IsPeakHour',
  ]
  numerical_features_to_scale_existing = [ f for f in numerical_features_to_scale if f in new_data_df_processed.columns ]

  # å…ˆæŠŠæ¬„ä½è½‰æˆ floatï¼Œé¿å… dtype ä¸ç›¸å®¹
  new_data_df_processed[numerical_features_to_scale_existing] = new_data_df_processed[numerical_features_to_scale_existing].astype(float)

  # æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µ
  scaled_values = scaler.transform(new_data_df_processed[numerical_features_to_scale_existing])
  scaled_df = pd.DataFrame(scaled_values, columns = numerical_features_to_scale_existing, index = new_data_df_processed.index)
  new_data_df_processed.loc[:, numerical_features_to_scale_existing] = scaled_df

  # è£œé½Šæ¬„ä½é †åºä¸¦å¡«é›¶
  X_new_final_ordered = pd.DataFrame(0, index = new_data_df_processed.index, columns = feature_names)
  for col in feature_names:
    if col in new_data_df_processed.columns:
      X_new_final_ordered[col] = new_data_df_processed[col]

  return X_new_final_ordered.values.astype(float)


# ä¸»ç¨‹å¼
def main():
  base_dir = "."
  vd_folders = [ 'VLRJM60', 'VLRJX00', 'VLRJX20']
  date_file = '2025-02-17_2025-02-23.json'

  # åˆä½µå¤šå€‹åµæ¸¬å™¨è³‡æ–™ç‚ºä¸€å€‹ DataFrame
  merged_df = combine_vd_dataframes(base_dir, vd_folders, date_file)
  if merged_df is None:
    print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½• VD çš„è³‡æ–™ã€‚")
    return

  print(f"åˆä½µå¾Œçš„ DataFrame è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)}")
  print(f"åˆä½µå¾Œçš„ DataFrame æ¬„ä½ï¼š{merged_df.columns}")
  print("-" * 80)

  # é€²è¡Œç‰¹å¾µå·¥ç¨‹èˆ‡ç›®æ¨™è®Šæ•¸åˆ†é›¢
  X, y, original_indices, feature_names, df = prepare_features(merged_df, is_training = True, return_indices = True)

  if X is None or y is None or feature_names is None:
    print("âŒ è³‡æ–™å‰è™•ç†å¤±æ•—ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
    return

  X = X.astype(float)
  y = y.astype(float)

  use_split = True  # True ä»£è¡¨ç”¨è¨“ç·´æ¸¬è©¦åˆ†å‰²ï¼ŒFalse ä»£è¡¨å…¨éƒ¨è¨“ç·´

  if use_split:
    # åˆ†å‰²æˆè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        shuffle=False,
    )
    print("ğŸ“¦ ä½¿ç”¨åˆ†å‰²æ–¹å¼è¨“ç·´")
  else:
    X_train, y_train = X, y
    X_test, y_test = X, y
    print("ğŸ“¦ ä½¿ç”¨ä¸åˆ†å‰²æ–¹å¼è¨“ç·´")

  model_path = './traffic_models/trained_model.keras'
  scaler_path = './traffic_models/scaler.pkl'

  # è¼‰å…¥å·²è¨“ç·´æ¨¡å‹ï¼Œæ²’æœ‰å‰‡å»ºç«‹æ–°æ¨¡å‹
  if os.path.exists(model_path):
    model = load_model(model_path, custom_objects = { 'mse': tf.keras.losses.MeanSquaredError})
    print("âœ… å·²åŠ è¼‰å…ˆå‰è¨“ç·´çš„æ¨¡å‹")
    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mse')
  else:
    print("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„æ¨¡å‹")
    print(f"âš ï¸ å»ºç«‹æ–°æ¨¡å‹ï¼Œè¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼š{X.shape[1]}")  # 26 å€‹ç‰¹å¾µ
    model = build_model(input_shape = X.shape[1])
    print("ğŸ†• å»ºç«‹æ–°çš„æ¨¡å‹")

  # è¼‰å…¥ scaler ç”¨æ–¼æ¨™æº–åŒ–
  scaler = None
  if os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰ä½¿ç”¨çš„ç‰¹å¾µç¸®æ”¾å™¨")
  else:
    if os.path.exists(model_path):
      print("âš ï¸ è¼‰å…¥ç‰¹å¾µç¸®æ”¾å™¨å¤±æ•—ï¼šscaler.pkl æª”æ¡ˆä¸å­˜åœ¨ã€‚")

  # è¨“ç·´æ¨¡å‹
  print("â³ é–‹å§‹æ¨¡å‹è¨“ç·´...")
  train_model(model, X_train, y_train, epochs = 50)
  print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

  # å„²å­˜æ¨¡å‹
  model.save(model_path)
  print(f"âœ… æ¨¡å‹å·²å„²å­˜åˆ° {model_path}")

  # å„²å­˜ scaler æ¨™æº–åŒ–å™¨
  if scaler is not None:
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜åˆ° {scaler_path}")
  else:
    print("âš ï¸ scaler ç‚ºç©ºï¼Œç„¡æ³•å„²å­˜")

  # æ¨¡å‹è©•ä¼°
  print("ğŸ“Š é–‹å§‹æ¨¡å‹è©•ä¼°...")
  y_pred = evaluate_model(model, X_test, y_test)
  batch_predictions_scaled = predict_new(model, X_test)
  print("âœ… æ¨¡å‹è©•ä¼°å®Œæˆã€‚")

  # ç•«åœ–ç¯„ä¾‹ï¼ˆå¯ä»¥è§£é™¤è¨»è§£åŸ·è¡Œï¼‰
  # åæ¨™æº–åŒ– Occupancyï¼Œé€™æ¨£æ‰èƒ½é¡¯ç¤ºåŸæœ¬çš„ä½”ç”¨ç‡
  df_viz = df.copy()
  features_to_inverse = list(scaler.feature_names_in_)
  df_viz = inverse_transform_all(df, scaler, features_to_inverse)
  plot_occupancy_vs_green_seconds(df_viz)
  plot_occupancy_distribution(df_viz)
  plot_occupancy_time_trend(df_viz)

  # plot_volume_distribution(merged_df)
  # plot_speed_distribution(merged_df)
  # plot_residuals(y_test, y_pred)

  # SHAP
  print('-' * 80)
  explain_shap_feature(model, X_train, X_test, feature_names, output_dir = "shap")
  print('-' * 80)

  # âœ… å¾ JSON è®€å–å¤šç­†è³‡æ–™ä¸¦é æ¸¬
  samples_path = 'sample_inputs.json'
  if os.path.exists(samples_path) and scaler is not None:
    print("ğŸ“Œ æ‰¹æ¬¡é æ¸¬è³‡æ–™ï¼š")
    with open(samples_path, 'r') as f:
      samples_inputs = json.load(f)

      for i, sample_input in enumerate(samples_inputs):
        df = pd.DataFrame([sample_input])
        X_new = preprocess_and_scale_new_data(df, feature_names, scaler)
        pred = predict_new(model, X_new)

        # é¡¯ç¤ºæ™‚é–“èˆ‡æ˜¯å¦å°–å³°
        hour = sample_input.get('Hour', -1)
        minute = sample_input.get('Minute', -1)
        is_peak = sample_input.get('IsPeakHour', 0)
        peak_status = "å°–å³°" if is_peak == 1 else "é›¢å³°"

        print(f"ç¯„ä¾‹ {i+1}ï¼šæ™‚é–“ {hour:02d}:{minute:02d}ï¼ˆ{peak_status}ï¼‰ â†’ é æ¸¬ç¶ ç‡ˆç§’æ•¸ = {pred[0][0]:.2f} ç§’")
  else:
    print("âš ï¸ ç„¡æ³•åŸ·è¡Œæ‰¹æ¬¡é æ¸¬ï¼Œå› ç‚ºæ‰¾ä¸åˆ° test_data.json æˆ–ç‰¹å¾µç¸®æ”¾å™¨æœªè¼‰å…¥ã€‚")


# åŸ·è¡Œä¸»ç¨‹å¼
if __name__ == "__main__":
  main()
