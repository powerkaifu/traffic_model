import numpy as np
import pandas as pd
import os
import json
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model  # type: ignore
import seaborn as sns
import matplotlib.pyplot as plt

# è‡ªå·±çš„æ¨¡çµ„
from data_process import combine_vd_dataframes
from feature_engineering import prepare_features, inverse_transform_all
from predictor import build_model, train_model, evaluate_test, evaluate_train, predict_new
from visualizer import *
from shap_utils import explain_shap_feature

pd.set_option('display.width', None)
pd.set_option('display.max_columns', None)


# ä¸»ç¨‹å¼
def main():
  base_dir = "."
  vd_folders = [ 'VLRJM60', 'VLRJX00', 'VLRJX20']
  # date_file = '2025-02-17_2025-02-23.json'
  # date_file = '2025-02-24_2025-03-02.json'
  # date_file = '2025-03-03_2025-03-09.json'
  # date_file = '2025-03-10_2025-03-16.json'
  # date_file = '2025-03-17_2025-03-23.json'
  # date_file = '2025-03-24_2025-03-30.json'
  # date_file = '2025-03-31_2025-04-06.json'
  # date_file = '2025-05-05_2025-05-11.json'
  date_file = '2025-06-02_2025-06-08.json'

  # --- è®€å–æœ¬é€±è³‡æ–™ ---
  merged_df = combine_vd_dataframes(base_dir, vd_folders, date_file)
  if merged_df is None:
    print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½• VD çš„è³‡æ–™ã€‚")
    return

  # --- è®€å–ä¸Šä¸€é€±è³‡æ–™ ---
  # ç‚ºäº†é¿å…ç¥ç¶“ç¶²è·¯åœ¨å¢é‡è¨“ç·´æ™‚ç™¼ç”Ÿã€Œç½é›£æ€§éºå¿˜ï¼ˆCatastrophic Forgettingï¼‰ã€ï¼Œ
  # ä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨å­¸ç¿’æ–°è³‡æ–™æ™‚å¿˜è¨˜äº†ä¹‹å‰å·²å­¸éçš„çŸ¥è­˜ï¼Œ
  # æˆ‘å€‘æœƒä¿ç•™éƒ¨åˆ†èˆŠè³‡æ–™ï¼ˆä¸Šä¸€é€±çš„è³‡æ–™ï¼‰ä¸€èµ·æ··åˆè¨“ç·´ã€‚
  # é€™æ¨£å¯ä»¥è®“æ¨¡å‹åœ¨é¢å°æ–°æ•¸æ“šæ™‚ï¼ŒåŒæ™‚ä¿æŒå°éå»è³‡æ–™çš„è¨˜æ†¶ï¼Œæå‡æ³›åŒ–èƒ½åŠ›èˆ‡ç©©å®šæ€§ã€‚
  # å› ç‚ºç›´æ¥æŠŠæ‰€æœ‰èˆŠè³‡æ–™éƒ½åŠ é€²å»è¨“ç·´æœƒå°è‡´è³‡æ–™é‡éå¤§ï¼Œè¨“ç·´æˆæœ¬ä¸Šå‡ï¼Œ
  # æ‰€ä»¥æ¡ç”¨æŠ½æ¨£æ–¹å¼ï¼Œå–éƒ¨åˆ†èˆŠè³‡æ–™ï¼Œæ¸›å°‘è³‡æ–™é‡åŒæ™‚ä¿æŒèˆŠçŸ¥è­˜ã€‚
  # é€™ç¨®åšæ³•åœ¨å¢é‡å­¸ç¿’èˆ‡æŒçºŒå­¸ç¿’ä¸­éå¸¸å¸¸è¦‹ï¼Œå¯ä»¥æœ‰æ•ˆç·©è§£æ¨¡å‹å¿˜è¨˜èˆŠè³‡æ–™çš„å•é¡Œã€‚
  date_file_last_week = '2025-05-05_2025-05-11.json'  # è«‹æ ¹æ“šå¯¦éš›ä¸Šä¸€é€±æª”åèª¿æ•´
  merged_df_last_week = combine_vd_dataframes(base_dir, vd_folders, date_file_last_week)

  if merged_df_last_week is not None:
    # æŠ½æ¨£èˆŠè³‡æ–™ 20%ï¼ˆå¯ä¾éœ€æ±‚èª¿æ•´æ¯”ä¾‹ï¼‰
    sample_ratio = 0.2
    sampled_old_df = merged_df_last_week.sample(frac = sample_ratio, random_state = 42)
    # åˆä½µæœ¬é€±èˆ‡æŠ½æ¨£å¾Œçš„èˆŠè³‡æ–™
    merged_df = pd.concat([ merged_df, sampled_old_df ], ignore_index = True)
    print(f"åˆä½µæœ¬é€±èˆ‡ä¸Šä¸€é€±æŠ½æ¨£è³‡æ–™ï¼Œç¸½ç­†æ•¸ï¼š{len(merged_df)}")
  else:
    print("âš ï¸ æ²’æœ‰è®€å–åˆ°ä¸Šä¸€é€±è³‡æ–™ï¼Œåƒ…ä½¿ç”¨æœ¬é€±è³‡æ–™è¨“ç·´ã€‚")

  print(f"åˆä½µå¾Œçš„ DataFrame è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)}")
  print(f"åˆä½µå¾Œçš„ DataFrame æ¬„ä½ï¼š{merged_df.columns}")
  print("-" * 80)

  # é€²è¡Œç‰¹å¾µå·¥ç¨‹ --------------------------------------------------------------------------------------------------------
  ## ä¸€èˆ¬ç‰¹å¾µæ¨™æº–åŒ–çš„ X æ•¸æ“šéƒ½è½æ–¼ -3 åˆ° 3 ä¹‹é–“
  X, y, original_indices, feature_names, df = prepare_features(merged_df, is_training = True, return_indices = True)

  print("y çš„å½¢ç‹€ï¼š", y.shape)  # æŸ¥çœ‹å½¢ç‹€ï¼ˆå¹¾ç­†è³‡æ–™ï¼‰
  print("y çš„å‰å¹¾ç­†è³‡æ–™ï¼š\n", y[: 10])  # é¡¯ç¤ºå‰ 10 ç­†
  print("y çš„æœ€å°å€¼ï¼š", np.min(y))
  print("y çš„æœ€å¤§å€¼ï¼š", np.max(y))

  # ç¢ºä¿æ˜¯ float é¡å‹
  X = X.astype(float)
  y = y.astype(float)

  # åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† -------------ã„¦------------------------------------------------------------------------------------
  # X_train, y_train => è¨“ç·´é›†
  # X_test, y_test => æ¸¬è©¦é›†
  X_train, X_test, y_train, y_test = train_test_split(
      X, y,
      test_size=0.2,
      random_state=42,
      shuffle=False,
  )

  print("ğŸ” è¨“ç·´é›†ç¶ ç‡ˆç§’æ•¸å€é–“")
  print("æœ€å°å€¼ï¼š", y_train.min())
  print("æœ€å¤§å€¼ï¼š", y_train.max())

  print("\nğŸ” æ¸¬è©¦é›†ç¶ ç‡ˆç§’æ•¸å€é–“")
  print("æœ€å°å€¼ï¼š", y_test.min())
  print("æœ€å¤§å€¼ï¼š", y_test.max())

  # æŸ¥çœ‹ y_train çš„åˆ†å¸ƒæƒ…å½¢
  print("ğŸ“Š y_train çµ±è¨ˆæ‘˜è¦ï¼š")
  print(pd.Series(y_train.flatten()).describe())

  # è¼‰å…¥æ¨¡å‹ï¼Œå¦å‰‡å»ºç«‹æ–°æ¨¡å‹ ----------------------------------------------------------------------------------------------
  model_path = './traffic_models/trained_model.keras'  # æ¨¡å‹å„²å­˜è·¯å¾‘
  scaler_path = './traffic_models/scaler.pkl'  # ç‰¹å¾µç¸®æ”¾å™¨å„²å­˜è·¯å¾‘
  if os.path.exists(model_path):
    model = load_model(model_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰è¨“ç·´çš„æ¨¡å‹")
    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mse', metrics = ['mae'])
  else:
    print("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„æ¨¡å‹")
    print(f"âš ï¸ å»ºç«‹æ–°æ¨¡å‹ï¼Œè¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼š{X.shape[1]}")  # 26 å€‹ç‰¹å¾µ
    model = build_model(input_shape = X.shape[1])
    print("ğŸ†• å»ºç«‹æ–°çš„æ¨¡å‹")

  # è¼‰å…¥ scaler ç”¨æ–¼æ¨™æº–åŒ– --------------------------------------------------------------------------------------------
  scaler = None
  if os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰ä½¿ç”¨çš„ç‰¹å¾µç¸®æ”¾å™¨")
  else:
    if os.path.exists(model_path):
      print("âš ï¸ è¼‰å…¥ç‰¹å¾µç¸®æ”¾å™¨å¤±æ•—ï¼šscaler.pkl æª”æ¡ˆä¸å­˜åœ¨ã€‚")

  # è¨“ç·´æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  print("â³ é–‹å§‹æ¨¡å‹è¨“ç·´...")
  history = train_model(model, X_train, y_train, epochs = 50)
  train_mae = history.history['mae']  # æ¯å€‹ epoch çš„è¨“ç·´ MAE
  val_mae = history.history.get('val_mae')  # æœ‰äº›ç‰ˆæœ¬æ²’é©—è­‰é›†å‰‡ç„¡æ­¤éµ
  if val_mae is not None:
    plot_mae(train_mae, val_mae)
  else:
    plot_mae(train_mae, [])

  print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

  # å„²å­˜æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  # model.save(model_path)
  # print(f"âœ… æ¨¡å‹å·²å„²å­˜åˆ° {model_path}")

  # å„²å­˜ scaler æ¨™æº–åŒ–å™¨ ----------------------------------------------------------------------------------------------
  if scaler is not None:
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜åˆ° {scaler_path}")
  else:
    print("âš ï¸ scaler ç‚ºç©ºï¼Œç„¡æ³•å„²å­˜")

  # æ¨¡å‹è©•ä¼° ---------------------------------------------------------------------------------------------------------
  print("ğŸ“Š é–‹å§‹æ¨¡å‹è©•ä¼°...è©•ä¼°è¨“ç·´é›†ã€æ¸¬è©¦é›†")
  y_pred_train = evaluate_train(model, X_train, y_train)
  y_pred_test = evaluate_test(model, X_test, y_test)
  # batch_predictions_scaled = predict_new(model, X_test)
  print("âœ… æ¨¡å‹è©•ä¼°å®Œæˆã€‚")

  # ç¹ªè£½åœ–è¡¨ ---------------------------------------------------------------------------------------------------------
  # åˆä½µè³‡æ–™çš„è³‡æ–™è¦–è¦ºåŒ–
  print(merged_df.columns)
  # plot_feature_distributions(merged_df, [ "Occupancy", "Speed", "Volume"])

  # merged_df['hour'] = pd.to_datetime(merged_df['timestamp']).dt.hour
  # plot_hourly_distributions(merged_df, [ "Occupancy", "Speed", "Volume_S"])

  # åæ¨™æº–åŒ– Occupancyï¼Œé€™æ‰èƒ½é¡¯ç¤ºåŸæœ¬çš„ä½”ç”¨ç‡ï¼ˆå› ç‚ºæ¨™æº–åŒ–å¾Œçš„ Occupancy æœƒè½åœ¨ -n åˆ° +n ä¹‹é–“ï¼‰
  df_viz = df.copy()
  features_to_inverse = list(scaler.feature_names_in_)
  df_viz = inverse_transform_all(df, scaler, features_to_inverse)
  # plot_occupancy_time_trend(df_viz)

  ## æ•£é»åœ–-è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
  fig, axs = plt.subplots(1, 2, figsize = ( 12, 5 ))  # ä¸€æ’å…©å¼µåœ–
  plot_scatter_predictions(y_train.flatten(), y_pred_train.flatten(), ax = axs[0], title = "è¨“ç·´é›†æ•£é»åœ–")
  plot_scatter_predictions(y_test.flatten(), y_pred_test.flatten(), ax = axs[1], title = "æ¸¬è©¦é›†æ•£é»åœ–")
  plt.tight_layout()
  plt.show()

  # è©•æ–·æ¨¡å‹æ³›åŒ–å¥½å£
  print("è¨“ç·´é›†é æ¸¬æœ€å°ç¶ ç‡ˆç§’æ•¸ï¼š", np.min(y_pred_train))
  print("è¨“ç·´é›†é æ¸¬æœ€å¤§ç¶ ç‡ˆç§’æ•¸ï¼š", np.max(y_pred_train))
  print("æ¸¬è©¦é›†é æ¸¬æœ€å°ç¶ ç‡ˆç§’æ•¸ï¼š", np.min(y_pred_test))
  print("æ¸¬è©¦é›†é æ¸¬æœ€å¤§ç¶ ç‡ˆç§’æ•¸ï¼š", np.max(y_pred_test))

  # èª¤å·®åˆ†å¸ƒåœ–
  # plot_residuals(y_test, y_pred_test)

  # SHAP
  # explain_shap_feature(model, X_train, X_test, feature_names, output_dir = "shap")


# åŸ·è¡Œä¸»ç¨‹å¼
if __name__ == "__main__":
  main()
