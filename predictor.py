import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential  # type: ignore
from tensorflow.keras.layers import Dense, Input, Dropout  # type: ignore
from tensorflow.keras.optimizers import Adam  # type: ignore
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # type: ignore
from tensorflow.keras import regularizers  # type: ignore
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ========================
# âœ… åŸå§‹ç‰ˆæœ¬ï¼šä¿ç•™è¨»è§£å®Œæ•´å‚™ä»½
# ========================

# å»ºç«‹æ¨¡å‹
# input_shape æ˜¯æ¨¡å‹çš„è¼¸å…¥å½¢ç‹€ï¼Œé€šå¸¸æ˜¯ç‰¹å¾µæ•¸é‡ï¼Œç›®å‰ 26ã€‚
# def build_model(input_shape):
#   model = Sequential([ # Sequential() æ˜¯ Keras ä¸­çš„ä¸€ç¨®æ¨¡å‹é¡å‹ï¼Œè¡¨ç¤ºä¸€å€‹ç·šæ€§å †ç–Šçš„ç¥ç¶“ç¶²è·¯ã€‚
#       Input(shape = ( input_shape,)),  # è¼¸å…¥å±¤ï¼Œå‘Šè¨´æ¨¡å‹è¼¸å…¥çš„ç‰¹å¾µæ•¸é‡
#       Dense(64, activation = 'relu'),  # ç¬¬ä¸€å±¤éš±è—å±¤ï¼Œæœ‰ 64 å€‹ç¥ç¶“å…ƒï¼ˆç¯€é»ï¼‰ï¼Œæ¯å€‹ç¥ç¶“å…ƒæœƒåŸ·è¡Œä¸€å€‹ç°¡å–®çš„è¨ˆç®—ï¼Œä¸¦ç”¨ ReLU éç·šæ€§æ¿€æ´»å‡½æ•¸ä¾†è®“æ¨¡å‹èƒ½å­¸ç¿’è¤‡é›œçš„è³‡æ–™æ¨¡å¼
#       Dense(32, activation = 'relu'),  # ç¬¬äºŒå±¤éš±è—å±¤ï¼Œæœ‰ 32 å€‹ç¥ç¶“å…ƒï¼ŒåŠŸèƒ½åŒä¸Š
#       Dense(1),  # è¼¸å‡ºå±¤ï¼Œç”¢ç”Ÿé æ¸¬å€¼ï¼Œåªæœ‰ä¸€å€‹ç¥ç¶“å…ƒ(é€£çºŒæ•¸å€¼->ç¶ ç‡ˆç§’æ•¸)
#   ])
#   # optimizer=Adam æ˜¯å„ªåŒ–å™¨ï¼Œç”¨ä¾†æ›´æ–°æ¨¡å‹æ¬Šé‡ï¼Œè®“æå¤±å‡½æ•¸ï¼ˆèª¤å·®ï¼‰è®Šå°
#   # learning_rate=0.001 æ˜¯æ›´æ–°çš„é€Ÿåº¦ï¼Œå¤ªå¤§å¯èƒ½ä¸ç©©å®šï¼Œå¤ªå°å­¸å¾—æ…¢
#   # loss='mse' æ˜¯æå¤±å‡½æ•¸ï¼Œç”¨ã€Œå‡æ–¹èª¤å·®ã€è¡¡é‡é æ¸¬å€¼å’ŒçœŸå¯¦å€¼å·®è·
#   model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'mse', metrics = ['mae'])
#   return model


# ========================
# âœ… å¼·åŒ–ç‰ˆæœ¬ï¼šæ”¯æ´ Dropoutã€L2 æ­£å‰‡åŒ–ã€åƒæ•¸è¨­å®š
# ========================
def build_model(input_shape, dropout_rate = 0.2, l2_factor = 0.001):
  model = Sequential([
      Input(shape = ( input_shape,)),
      Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(l2_factor)),
      Dropout(dropout_rate),
      Dense(32, activation = 'relu', kernel_regularizer = regularizers.l2(l2_factor)),
      Dropout(dropout_rate),
      Dense(1),
  ])
  model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'mse', metrics = ['mae'])
  return model


# ========================
# æ¨¡å‹è¨“ç·´
# ========================
def train_model(model, X_train, y_train, epochs = 50):
  early_stop = EarlyStopping(monitor = 'val_loss', patience = 10, min_delta = 0.001, restore_best_weights = True)

  reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 5, min_lr = 1e-6, verbose = 1)

  history = model.fit(X_train, y_train, epochs = epochs, batch_size = 32, validation_split = 0.2, callbacks = [ early_stop, reduce_lr ], verbose = 1)

  return history


# ========================
# è©•ä¼°è¨“ç·´èˆ‡æ¸¬è©¦é›†
# ========================
def evaluate_train(model, X_train, y_train):
  y_pred = model.predict(X_train)
  mse = mean_squared_error(y_train, y_pred)
  rmse = np.sqrt(mse)
  mae = mean_absolute_error(y_train, y_pred)
  r2 = r2_score(y_train, y_pred)

  print("è¨“ç·´é›†æ¨¡å‹è©•ä¼°çµæœï¼š")
  print(f"ğŸ“‰ MSE: {mse:.2f}")
  print(f"ğŸ“‰ RMSE: {rmse:.2f}")
  print(f"ğŸ“‰ MAE: {mae:.2f}")
  print(f"ğŸ“ˆ RÂ² : {r2:.4f}")

  return y_pred


def evaluate_test(model, X_test, y_test):
  y_pred = model.predict(X_test)
  mse = mean_squared_error(y_test, y_pred)
  rmse = np.sqrt(mse)
  mae = mean_absolute_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)

  print("æ¸¬è©¦é›†æ¨¡å‹è©•ä¼°çµæœï¼š")
  print(f"ğŸ“‰ MSE: {mse:.2f}")
  print(f"ğŸ“‰ RMSE: {rmse:.2f}")
  print(f"ğŸ“‰ MAE: {mae:.2f}")
  print(f"ğŸ“ˆ RÂ² : {r2:.4f}")

  return y_pred


# ========================
# é æ¸¬æ–°è³‡æ–™
# ========================
def predict_new(model, X_new, min_sec = 20, max_sec = 99, float_output = False):
  predictions = model.predict(X_new)
  if not float_output:
    predictions = np.round(predictions).astype(int)
  predictions = np.clip(predictions, min_sec, max_sec)
  return predictions
