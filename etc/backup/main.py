import numpy as np
import pandas as pd
import os
import json
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model  # type: ignore
import seaborn as sns
import matplotlib.pyplot as plt

# è‡ªå·±çš„æ¨¡çµ„
from data_process import combine_vd_dataframes
from feature_engineering import prepare_features, inverse_transform_all
from predictor import build_model, train_model, evaluate_test, evaluate_train, predict_new
from visualizer import *
from shap_utils import explain_shap_feature

pd.set_option('display.width', None)
pd.set_option('display.max_columns', None)


# ä¸»ç¨‹å¼
def main():
  base_dir = "."
  vd_folders = [ 'VLRJM60', 'VLRJX00', 'VLRJX20']
  # date_file = '2025-02-17_2025-02-23.json'
  # date_file = '2025-02-24_2025-03-02.json'
  # date_file = '2025-03-03_2025-03-09.json'
  # date_file = '2025-03-10_2025-03-16.json'
  # date_file = '2025-03-17_2025-03-23.json'
  # date_file = '2025-03-24_2025-03-30.json'
  # date_file = '2025-03-31_2025-04-06.json'
  date_file = '2025-05-05_2025-05-11.json'
  # date_file = '2025-06-02_2025-06-08.json'

  # åˆä½µå¤šå€‹åµæ¸¬å™¨è³‡æ–™ç‚ºä¸€å€‹ DataFrame
  merged_df = combine_vd_dataframes(base_dir, vd_folders, date_file)
  if merged_df is None:
    print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½• VD çš„è³‡æ–™ã€‚")
    return

  print(f"åˆä½µå¾Œçš„ DataFrame è³‡æ–™ç­†æ•¸ï¼š{len(merged_df)}")
  print(f"åˆä½µå¾Œçš„ DataFrame æ¬„ä½ï¼š{merged_df.columns}")
  print("-" * 80)

  # é€²è¡Œç‰¹å¾µå·¥ç¨‹ --------------------------------------------------------------------------------------------------------
  ## ä¸€èˆ¬ç‰¹å¾µæ¨™æº–åŒ–çš„ X æ•¸æ“šéƒ½è½æ–¼ -3 åˆ° 3 ä¹‹é–“
  X, y, original_indices, feature_names, df = prepare_features(merged_df, is_training = True, return_indices = True)

  print("y çš„å½¢ç‹€ï¼š", y.shape)  # æŸ¥çœ‹å½¢ç‹€ï¼ˆå¹¾ç­†è³‡æ–™ï¼‰
  print("y çš„å‰å¹¾ç­†è³‡æ–™ï¼š\n", y[: 10])  # é¡¯ç¤ºå‰ 10 ç­†
  print("y çš„æœ€å°å€¼ï¼š", np.min(y))
  print("y çš„æœ€å¤§å€¼ï¼š", np.max(y))

  # ç¢ºä¿æ˜¯ float é¡å‹
  X = X.astype(float)
  y = y.astype(float)

  # åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›† -------------ã„¦------------------------------------------------------------------------------------
  # X_train, y_train => è¨“ç·´é›†
  # X_test, y_test => æ¸¬è©¦é›†
  X_train, X_test, y_train, y_test = train_test_split(
      X, y,
      test_size=0.2,
      random_state=42,
      shuffle=False,
  )

  print("ğŸ” è¨“ç·´é›†ç¶ ç‡ˆç§’æ•¸å€é–“")
  print("æœ€å°å€¼ï¼š", y_train.min())
  print("æœ€å¤§å€¼ï¼š", y_train.max())

  print("\nğŸ” æ¸¬è©¦é›†ç¶ ç‡ˆç§’æ•¸å€é–“")
  print("æœ€å°å€¼ï¼š", y_test.min())
  print("æœ€å¤§å€¼ï¼š", y_test.max())

  # æŸ¥çœ‹ y_train çš„åˆ†å¸ƒæƒ…å½¢
  print("ğŸ“Š y_train çµ±è¨ˆæ‘˜è¦ï¼š")
  print(pd.Series(y_train.flatten()).describe())

  # è¼‰å…¥æ¨¡å‹ï¼Œå¦å‰‡å»ºç«‹æ–°æ¨¡å‹ ----------------------------------------------------------------------------------------------
  model_path = './traffic_models/trained_model.keras'  # æ¨¡å‹å„²å­˜è·¯å¾‘
  scaler_path = './traffic_models/scaler.pkl'  # ç‰¹å¾µç¸®æ”¾å™¨å„²å­˜è·¯å¾‘
  if os.path.exists(model_path):
    model = load_model(model_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰è¨“ç·´çš„æ¨¡å‹")
    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mse')
  else:
    print("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„æ¨¡å‹")
    print(f"âš ï¸ å»ºç«‹æ–°æ¨¡å‹ï¼Œè¼¸å…¥ç‰¹å¾µæ•¸é‡ï¼š{X.shape[1]}")  # 26 å€‹ç‰¹å¾µ
    model = build_model(input_shape = X.shape[1])
    print("ğŸ†• å»ºç«‹æ–°çš„æ¨¡å‹")

  # è¼‰å…¥ scaler ç”¨æ–¼æ¨™æº–åŒ– --------------------------------------------------------------------------------------------
  scaler = None
  if os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
    print("âœ… å·²åŠ è¼‰å…ˆå‰ä½¿ç”¨çš„ç‰¹å¾µç¸®æ”¾å™¨")
  else:
    if os.path.exists(model_path):
      print("âš ï¸ è¼‰å…¥ç‰¹å¾µç¸®æ”¾å™¨å¤±æ•—ï¼šscaler.pkl æª”æ¡ˆä¸å­˜åœ¨ã€‚")

  # è¨“ç·´æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  print("â³ é–‹å§‹æ¨¡å‹è¨“ç·´...")
  train_model(model, X_train, y_train, epochs = 50)
  print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆã€‚")

  # å„²å­˜æ¨¡å‹ ---------------------------------------------------------------------------------------------------------
  # model.save(model_path)
  # print(f"âœ… æ¨¡å‹å·²å„²å­˜åˆ° {model_path}")

  # å„²å­˜ scaler æ¨™æº–åŒ–å™¨ ----------------------------------------------------------------------------------------------
  if scaler is not None:
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ç‰¹å¾µç¸®æ”¾å™¨å·²å„²å­˜åˆ° {scaler_path}")
  else:
    print("âš ï¸ scaler ç‚ºç©ºï¼Œç„¡æ³•å„²å­˜")

  # æ¨¡å‹è©•ä¼° ---------------------------------------------------------------------------------------------------------
  print("ğŸ“Š é–‹å§‹æ¨¡å‹è©•ä¼°...è©•ä¼°è¨“ç·´é›†ã€æ¸¬è©¦é›†")
  y_pred_train = evaluate_train(model, X_train, y_train)
  y_pred_test = evaluate_test(model, X_test, y_test)
  # batch_predictions_scaled = predict_new(model, X_test)
  print("âœ… æ¨¡å‹è©•ä¼°å®Œæˆã€‚")

  # ç¹ªè£½åœ–è¡¨ ---------------------------------------------------------------------------------------------------------
  # åˆä½µè³‡æ–™çš„è³‡æ–™è¦–è¦ºåŒ–
  print(merged_df.columns)
  # plot_feature_distributions(merged_df, [ "Occupancy", "Speed", "Volume"])

  # merged_df['hour'] = pd.to_datetime(merged_df['timestamp']).dt.hour
  # plot_hourly_distributions(merged_df, [ "Occupancy", "Speed", "Volume_S"])

  # åæ¨™æº–åŒ– Occupancyï¼Œé€™æ‰èƒ½é¡¯ç¤ºåŸæœ¬çš„ä½”ç”¨ç‡ï¼ˆå› ç‚ºæ¨™æº–åŒ–å¾Œçš„ Occupancy æœƒè½åœ¨ -n åˆ° +n ä¹‹é–“ï¼‰
  df_viz = df.copy()
  features_to_inverse = list(scaler.feature_names_in_)
  df_viz = inverse_transform_all(df, scaler, features_to_inverse)
  # plot_occupancy_time_trend(df_viz)

  ## æ•£é»åœ–-è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
  fig, axs = plt.subplots(1, 2, figsize = ( 12, 5 ))  # ä¸€æ’å…©å¼µåœ–
  plot_scatter_predictions(y_train.flatten(), y_pred_train.flatten(), ax = axs[0], title = "è¨“ç·´é›†æ•£é»åœ–")
  plot_scatter_predictions(y_test.flatten(), y_pred_test.flatten(), ax = axs[1], title = "æ¸¬è©¦é›†æ•£é»åœ–")
  plt.tight_layout()
  plt.show()

  print("è¨“ç·´é›†é æ¸¬æœ€å°ç¶ ç‡ˆç§’æ•¸ï¼š", np.min(y_pred_train))
  print("è¨“ç·´é›†é æ¸¬æœ€å¤§ç¶ ç‡ˆç§’æ•¸ï¼š", np.max(y_pred_train))
  print("æ¸¬è©¦é›†é æ¸¬æœ€å°ç¶ ç‡ˆç§’æ•¸ï¼š", np.min(y_pred_test))
  print("æ¸¬è©¦é›†é æ¸¬æœ€å¤§ç¶ ç‡ˆç§’æ•¸ï¼š", np.max(y_pred_test))

  # èª¤å·®åˆ†å¸ƒåœ–
  # plot_residuals(y_test, y_pred_test)

  # SHAP
  # explain_shap_feature(model, X_train, X_test, feature_names, output_dir = "shap")


# åŸ·è¡Œä¸»ç¨‹å¼
if __name__ == "__main__":
  main()
